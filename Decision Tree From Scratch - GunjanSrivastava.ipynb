{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Decision Tree From Scratch - Anupama Kurudi.ipynb","provenance":[],"mount_file_id":"1t1XQ1DqcMM2GtM69e4rfcIIOONioGHzA","authorship_tag":"ABX9TyNASPdl4ZyD4idQpQ/b8H3h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Decision Tree Algorithm in Python From Scratch**\n"],"metadata":{"id":"lITADc4sryw8"}},{"cell_type":"code","source":["# Data wrangling \n","import pandas as pd \n","\n","# Array math\n","import numpy as np \n","\n","# Quick value count calculator\n","from collections import Counter"],"metadata":{"id":"-y9y29ROppJq","executionInfo":{"status":"ok","timestamp":1639094594751,"user_tz":480,"elapsed":6,"user":{"displayName":"Anupama Kurudi Narasimha Murthy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11511651901317411562"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"sC4phbEipXQa","executionInfo":{"status":"ok","timestamp":1639094770603,"user_tz":480,"elapsed":769,"user":{"displayName":"Anupama Kurudi Narasimha Murthy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11511651901317411562"}}},"outputs":[],"source":["class Node: \n","    \"\"\"\n","    Class for creating the nodes for a decision tree \n","    \"\"\"\n","    def __init__(\n","        self, \n","        Y: list,\n","        X: pd.DataFrame,\n","        min_samples_split=None,\n","        max_depth=None,\n","        depth=None,\n","        node_type=None,\n","        rule=None\n","    ):\n","        # Saving the data to the node \n","        self.Y = Y \n","        self.X = X\n","\n","        # Saving the hyper parameters\n","        self.min_samples_split = min_samples_split if min_samples_split else 20\n","        self.max_depth = max_depth if max_depth else 5\n","\n","        # Default current depth of node \n","        self.depth = depth if depth else 0\n","\n","        # Extracting all the features\n","        self.features = list(self.X.columns)\n","\n","        # Type of node \n","        self.node_type = node_type if node_type else 'root'\n","\n","        # Rule for spliting \n","        self.rule = rule if rule else \"\"\n","\n","        # Calculating the counts of Y in the node \n","        self.counts = Counter(Y)\n","\n","        # Getting the GINI impurity based on the Y distribution\n","        self.gini_impurity = self.get_GINI()\n","\n","        # Sorting the counts and saving the final prediction of the node \n","        counts_sorted = list(sorted(self.counts.items(), key=lambda item: item[1]))\n","\n","        # Getting the last item\n","        yhat = None\n","        if len(counts_sorted) > 0:\n","            yhat = counts_sorted[-1][0]\n","\n","        # Saving to object attribute. This node will predict the class with the most frequent class\n","        self.yhat = yhat \n","\n","        # Saving the number of observations in the node \n","        self.n = len(Y)\n","\n","        # Initiating the left and right nodes as empty nodes\n","        self.left = None \n","        self.right = None \n","\n","        # Default values for splits\n","        self.best_feature = None \n","        self.best_value = None \n","\n","    @staticmethod\n","    def GINI_impurity(y1_count: int, y2_count: int) -> float:\n","        \"\"\"\n","        Given the observations of a binary class calculate the GINI impurity\n","        \"\"\"\n","        # Ensuring the correct types\n","        if y1_count is None:\n","            y1_count = 0\n","\n","        if y2_count is None:\n","            y2_count = 0\n","\n","        # Getting the total observations\n","        n = y1_count + y2_count\n","        \n","        # If n is 0 then we return the lowest possible gini impurity\n","        if n == 0:\n","            return 0.0\n","\n","        # Getting the probability to see each of the classes\n","        p1 = y1_count / n\n","        p2 = y2_count / n\n","        \n","        # Calculating GINI \n","        gini = 1 - (p1 ** 2 + p2 ** 2)\n","        \n","        # Returning the gini impurity\n","        return gini\n","\n","    @staticmethod\n","    def ma(x: np.array, window: int) -> np.array:\n","        \"\"\"\n","        Calculates the moving average of the given list. \n","        \"\"\"\n","        return np.convolve(x, np.ones(window), 'valid') / window\n","\n","    def get_GINI(self):\n","        \"\"\"\n","        Function to calculate the GINI impurity of a node \n","        \"\"\"\n","        # Getting the 0 and 1 counts\n","        y1_count, y2_count = self.counts.get(0, 0), self.counts.get(1, 0)\n","\n","        # Getting the GINI impurity\n","        return self.GINI_impurity(y1_count, y2_count)\n","\n","    def best_split(self) -> tuple:\n","        \"\"\"\n","        Given the X features and Y targets calculates the best split \n","        for a decision tree\n","        \"\"\"\n","        # Creating a dataset for spliting\n","        df = self.X.copy()\n","        df['Y'] = self.Y\n","\n","        # Getting the GINI impurity for the base input \n","        GINI_base = self.get_GINI()\n","\n","        # Finding which split yields the best GINI gain \n","        max_gain = 0\n","\n","        # Default best feature and split\n","        best_feature = None\n","        best_value = None\n","\n","        for feature in self.features:\n","            # Droping missing values\n","            Xdf = df.dropna().sort_values(feature)\n","\n","            # Sorting the values and getting the rolling average\n","            xmeans = self.ma(Xdf[feature].unique(), 2)\n","\n","            for value in xmeans:\n","                # Spliting the dataset \n","                left_counts = Counter(Xdf[Xdf[feature]<value]['Y'])\n","                right_counts = Counter(Xdf[Xdf[feature]>=value]['Y'])\n","\n","                # Getting the Y distribution from the dicts\n","                y0_left, y1_left, y0_right, y1_right = left_counts.get(0, 0), left_counts.get(1, 0), right_counts.get(0, 0), right_counts.get(1, 0)\n","\n","                # Getting the left and right gini impurities\n","                gini_left = self.GINI_impurity(y0_left, y1_left)\n","                gini_right = self.GINI_impurity(y0_right, y1_right)\n","\n","                # Getting the obs count from the left and the right data splits\n","                n_left = y0_left + y1_left\n","                n_right = y0_right + y1_right\n","\n","                # Calculating the weights for each of the nodes\n","                w_left = n_left / (n_left + n_right)\n","                w_right = n_right / (n_left + n_right)\n","\n","                # Calculating the weighted GINI impurity\n","                wGINI = w_left * gini_left + w_right * gini_right\n","\n","                # Calculating the GINI gain \n","                GINIgain = GINI_base - wGINI\n","\n","                # Checking if this is the best split so far \n","                if GINIgain > max_gain:\n","                    best_feature = feature\n","                    best_value = value \n","\n","                    # Setting the best gain to the current one \n","                    max_gain = GINIgain\n","\n","        return (best_feature, best_value)\n","\n","    def grow_tree(self):\n","        \"\"\"\n","        Recursive method to create the decision tree\n","        \"\"\"\n","        # Making a df from the data \n","        df = self.X.copy()\n","        df['Y'] = self.Y\n","\n","        # If there is GINI to be gained, we split further \n","        if (self.depth < self.max_depth) and (self.n >= self.min_samples_split):\n","\n","            # Getting the best split \n","            best_feature, best_value = self.best_split()\n","\n","            if best_feature is not None:\n","                # Saving the best split to the current node \n","                self.best_feature = best_feature\n","                self.best_value = best_value\n","\n","                # Getting the left and right nodes\n","                left_df, right_df = df[df[best_feature]<=best_value].copy(), df[df[best_feature]>best_value].copy()\n","\n","                # Creating the left and right nodes\n","                left = Node(\n","                    left_df['Y'].values.tolist(), \n","                    left_df[self.features], \n","                    depth=self.depth + 1, \n","                    max_depth=self.max_depth, \n","                    min_samples_split=self.min_samples_split, \n","                    node_type='left_node',\n","                    rule=f\"{best_feature} <= {round(best_value, 3)}\"\n","                    )\n","\n","                self.left = left \n","                self.left.grow_tree()\n","\n","                right = Node(\n","                    right_df['Y'].values.tolist(), \n","                    right_df[self.features], \n","                    depth=self.depth + 1, \n","                    max_depth=self.max_depth, \n","                    min_samples_split=self.min_samples_split,\n","                    node_type='right_node',\n","                    rule=f\"{best_feature} > {round(best_value, 3)}\"\n","                    )\n","\n","                self.right = right\n","                self.right.grow_tree()\n","\n","    def print_info(self, width=4):\n","        \"\"\"\n","        Method to print the infromation about the tree\n","        \"\"\"\n","        # Defining the number of spaces \n","        const = int(self.depth * width ** 1.5)\n","        spaces = \"-\" * const\n","        \n","        if self.node_type == 'root':\n","            print(\"Root\")\n","        else:\n","            print(f\"|{spaces} Split rule: {self.rule}\")\n","        print(f\"{' ' * const}   | GINI impurity of the node: {round(self.gini_impurity, 2)}\")\n","        print(f\"{' ' * const}   | Class distribution in the node: {dict(self.counts)}\")\n","        print(f\"{' ' * const}   | Predicted class: {self.yhat}\")   \n","\n","    def print_tree(self):\n","        \"\"\"\n","        Prints the whole tree from the current node to the bottom\n","        \"\"\"\n","        self.print_info() \n","        \n","        if self.left is not None: \n","            self.left.print_tree()\n","        \n","        if self.right is not None:\n","            self.right.print_tree()\n","\n","    def predict(self, X:pd.DataFrame):\n","        \"\"\"\n","        Batch prediction method\n","        \"\"\"\n","        predictions = []\n","\n","        for _, x in X.iterrows():\n","            values = {}\n","            for feature in self.features:\n","                values.update({feature: x[feature]})\n","        \n","            predictions.append(self.predict_obs(values))\n","        \n","        return predictions\n","\n","    def predict_obs(self, values: dict) -> int:\n","        \"\"\"\n","        Method to predict the class given a set of features\n","        \"\"\"\n","        cur_node = self\n","        while cur_node.depth < cur_node.max_depth:\n","            # Traversing the nodes all the way to the bottom\n","            best_feature = cur_node.best_feature\n","            best_value = cur_node.best_value\n","\n","            if cur_node.n < cur_node.min_samples_split:\n","                break \n","\n","            if (values.get(best_feature) < best_value):\n","                if self.left is not None:\n","                    cur_node = cur_node.left\n","            else:\n","                if self.right is not None:\n","                    cur_node = cur_node.right\n","            \n","        return cur_node.yhat"]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # Reading data\n","    d = pd.read_csv(\"/content/drive/MyDrive/DM datasets/train.csv\")[['Age', 'Fare', 'Survived']].dropna()\n","\n","    # Constructing the X and Y matrices\n","    X = d[['Age', 'Fare']]\n","    Y = d['Survived'].values.tolist()\n","\n","    # Initiating the Node\n","    root = Node(Y, X, max_depth=3, min_samples_split=100)\n","\n","    # Getting teh best split\n","    root.grow_tree()\n","\n","    # Printing the tree information \n","    root.print_tree()\n","\n","    # Predicting \n","    Xsubset = X.copy()\n","    Xsubset['yhat'] = root.predict(Xsubset)\n","    print(Xsubset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U3XeyR5UqWZL","executionInfo":{"status":"ok","timestamp":1639095013840,"user_tz":480,"elapsed":1674,"user":{"displayName":"Anupama Kurudi Narasimha Murthy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11511651901317411562"}},"outputId":"b4619c88-3895-4038-b0b7-f7c779963ece"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Root\n","   | GINI impurity of the node: 0.48\n","   | Class distribution in the node: {0: 424, 1: 290}\n","   | Predicted class: 0\n","|-------- Split rule: Fare <= 52.277\n","           | GINI impurity of the node: 0.44\n","           | Class distribution in the node: {0: 389, 1: 195}\n","           | Predicted class: 0\n","|---------------- Split rule: Fare <= 10.481\n","                   | GINI impurity of the node: 0.32\n","                   | Class distribution in the node: {0: 192, 1: 47}\n","                   | Predicted class: 0\n","|------------------------ Split rule: Age <= 32.5\n","                           | GINI impurity of the node: 0.37\n","                           | Class distribution in the node: {0: 134, 1: 43}\n","                           | Predicted class: 0\n","|------------------------ Split rule: Age > 32.5\n","                           | GINI impurity of the node: 0.12\n","                           | Class distribution in the node: {0: 58, 1: 4}\n","                           | Predicted class: 0\n","|---------------- Split rule: Fare > 10.481\n","                   | GINI impurity of the node: 0.49\n","                   | Class distribution in the node: {0: 197, 1: 148}\n","                   | Predicted class: 0\n","|------------------------ Split rule: Age <= 6.5\n","                           | GINI impurity of the node: 0.41\n","                           | Class distribution in the node: {0: 12, 1: 30}\n","                           | Predicted class: 1\n","|------------------------ Split rule: Age > 6.5\n","                           | GINI impurity of the node: 0.48\n","                           | Class distribution in the node: {0: 185, 1: 118}\n","                           | Predicted class: 0\n","|-------- Split rule: Fare > 52.277\n","           | GINI impurity of the node: 0.39\n","           | Class distribution in the node: {1: 95, 0: 35}\n","           | Predicted class: 1\n","|---------------- Split rule: Age <= 63.5\n","                   | GINI impurity of the node: 0.38\n","                   | Class distribution in the node: {1: 95, 0: 32}\n","                   | Predicted class: 1\n","|------------------------ Split rule: Age <= 29.5\n","                           | GINI impurity of the node: 0.44\n","                           | Class distribution in the node: {0: 17, 1: 34}\n","                           | Predicted class: 1\n","|------------------------ Split rule: Age > 29.5\n","                           | GINI impurity of the node: 0.32\n","                           | Class distribution in the node: {1: 61, 0: 15}\n","                           | Predicted class: 1\n","|---------------- Split rule: Age > 63.5\n","                   | GINI impurity of the node: 0.0\n","                   | Class distribution in the node: {0: 3}\n","                   | Predicted class: 0\n","      Age     Fare  yhat\n","0    22.0   7.2500     0\n","1    38.0  71.2833     1\n","2    26.0   7.9250     0\n","3    35.0  53.1000     1\n","4    35.0   8.0500     0\n","..    ...      ...   ...\n","885  39.0  29.1250     0\n","886  27.0  13.0000     0\n","887  19.0  30.0000     0\n","889  26.0  30.0000     0\n","890  32.0   7.7500     0\n","\n","[714 rows x 3 columns]\n"]}]},{"cell_type":"markdown","source":["### The above result is the Decision Tree"],"metadata":{"id":"iT2Jfv7jrdvz"}}]}